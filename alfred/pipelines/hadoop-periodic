job {
    // Name of the pipeline to be created
    name = "hadoop-periodic-thunderX"

    // Type of pipeline periodic, on-demand and per-commit
    pipeline_type = "periodic"

    // Name of Workload
    workload = "hadoop"

    // Repos to be cloned for the pipeline
    clone_repos = "ats"

    // Cron value for periodic pipeline  to be triggered (only needed for periodic
    // pipelines) Possible options (@daily, @weekly, @monthly and cron format
    schedule = "@daily"

    // Agents to be assigned to the pipeline
    label = "hadoop_agents"

    // Scripts to be run in different stages
    stage1_bootstrap      = "./common/bootstrap.sh --workload hadoop"
    stage2_PreRunCleanup  = "./common/cleanup.sh --workload hadoop --soft"
    stage3_BuildHadoop    = "./common/stage.sh --workload hadoop --name build_hadoop"
    stage4_SetupHadoop    = "./common/stage.sh --workload hadoop --name setup_hadoop"
    stage5_BuildHibench   = "./common/stage.sh --workload hadoop --name build_hibench"
    stage6_SetupHibench   = "./common/stage.sh --workload hadoop --name setup_hibench"
    stage7_RunTests       = "./hadoop/tests/run_tests.sh"
    stage8_PostRunCleanup = "./common/cleanup.sh --workload hadoop --soft"

    // Alert build status of each job on specified email address
    email = ""

    // Common logs aggregation location for the job
    artifacts = "logs/"
}
